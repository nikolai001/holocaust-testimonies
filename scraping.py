from urllib import request, parsefrom http import cookiejarfrom bs4 import BeautifulSoup# Define headersheaders = {    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",    "Accept-Language": "en-US,en;q=0.5",    "Accept-Encoding": "utf-8",    "Connection": "keep-alive",    "Upgrade-Insecure-Requests": "1",    "Sec-Fetch-Dest": "document",    "Sec-Fetch-Mode": "navigate",    "Sec-Fetch-Site": "none",    "Sec-Fetch-User": "?1",    "Cache-Control": "max-age=0"}# Create a CookieJar object to store cookiescookie_jar = cookiejar.CookieJar()cookie_processor = request.HTTPCookieProcessor(cookie_jar)opener = request.build_opener(cookie_processor)url = "http://degob.org/index.php?listjk="params = {    'sex': "",    'place_birth': "",    'date_birth': "",    'occupation': "",    'residence': "",    'concentration': "",    'ghetto': "",    'camp': "",    'search': "ext"}# Encode the data and make the initial requestencoded_params = parse.urlencode(params).encode()req = request.Request(url, data=encoded_params, headers=headers)response = opener.open(req)resultsPage = BeautifulSoup(response.read(), features="lxml")entries = resultsPage.find('div',attrs={'id':'content'}).findAll('div',attrs={'align':'left'})maxPage = int(resultsPage.find('div', attrs={'id': 'content'}).find('div', attrs={'style': "margin-left:70px; margin-right:70px;"}).findAll('a', attrs={'class': "menu_main"})[-1].text)sub_url = "http://degob.org/index.php?listjk&page="for page in range(1, maxPage + 1):    subPageurl = str(sub_url)+str(page)    # First request has already been made to the page so we have the first few entries thus we start from index 2    req = request.Request(subPageurl, headers=headers)    response = opener.open(req)    currentPage = BeautifulSoup(response.read(), features="lxml")        entries = currentPage.find('div',attrs={'id':'content'}).find('div',attrs={'align':'left'}).findAll('div')    for entry in entries:        if entry.find('a'):            req = request.Request(str('http://degob.org/')+entry.find('a')['href'], headers = headers)            response = opener.open(req)            protocol = BeautifulSoup(response.read(),features="lxml")            fileName = protocol.find('h1').text.replace(' ','_').replace('.','_').replace('__','_')            f = open('raw-data/'+fileName+'.txt','w',encoding="utf-8")            f.write(str(protocol.decode('utf-8')))#for element in resultsPage:  #  link = resultsPage.find('div',attrs={'id':'content'}).find('div',attrs={'align':'left'})  #  nextButton = resultsPage.find('div',attrs={'id':'content'}).find('div',attrs={'align':'center'}).find('div',attrs={'align':'center','style':'width:150px;'}).find('a') #   print(nextButton)# request = urllib.request.Request( new_url, None, headers )# response = urllib.request.urlopen( request )# page = BeautifulSoup(response,features="lxml")# print(page)# print(new_url)# for movie in movies:#     link = movie.find('a')#     url = 'https://www.themoviedb.org'+link['href']#     try:#         request = urllib.request.Request( url, None, headers )#         response = urllib.request.urlopen( request )#         with open('data/'+link['title']+'.html', 'w',encoding="utf-8") as f:#             f.write(str(response.read().decode('utf-8')))#         time.sleep(1)#     except urllib.error.HTTPError:#         print("Page with ID "+str(id)+" found")