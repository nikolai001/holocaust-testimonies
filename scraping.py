from urllib import request, parsefrom http import cookiejarfrom bs4 import BeautifulSoup# Define headersheaders = {    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:98.0) Gecko/20100101 Firefox/98.0",    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",    "Accept-Language": "en-US,en;q=0.5",    "Accept-Encoding": "utf-8",    "Connection": "keep-alive",    "Upgrade-Insecure-Requests": "1",    "Sec-Fetch-Dest": "document",    "Sec-Fetch-Mode": "navigate",    "Sec-Fetch-Site": "none",    "Sec-Fetch-User": "?1",    "Cache-Control": "max-age=0"}# We use cookie_jar to use a session for our fetching of datacookie_jar = cookiejar.CookieJar()cookie_processor = request.HTTPCookieProcessor(cookie_jar)opener = request.build_opener(cookie_processor)url = "http://degob.org/index.php?listjk="params = {    'sex': "",    'place_birth': "",    'date_birth': "",    'occupation': "",    'residence': "",    'concentration': "",    'ghetto': "",    'camp': "",    'search': "ext"}# Encode the data and make the initial request# The reason we send parameters with it is because it's a POST request that requires# Data being sent, the reason we do this is to "trick" the server to return us# A list of all the availalbe english protocols but we're just sending empty values# except for the last valueencoded_params = parse.urlencode(params).encode()req = request.Request(url, data=encoded_params, headers=headers)response = opener.open(req)resultsPage = BeautifulSoup(response.read(), features="lxml")entries = resultsPage.find('div',attrs={'id':'content'}).findAll('div',attrs={'align':'left'})maxPage = int(resultsPage.find('div', attrs={'id': 'content'}).find('div', attrs={'style': "margin-left:70px; margin-right:70px;"}).findAll('a', attrs={'class': "menu_main"})[-1].text)# The reason we find the first page is to gain access to the maximum page value# and we want this to be dynamic should more protocols be translatedsub_url = "http://degob.org/index.php?listjk&page="for page in range(1, maxPage + 1):    subPageurl = str(sub_url)+str(page)        req = request.Request(subPageurl, headers=headers)    response = opener.open(req) # The reason we open the opener is because we    # want to use the current session that we opened before the loop because    # accessing the different pages requires the data that's fetched from the    # parameters we send in the initial request    currentPage = BeautifulSoup(response.read(), features="lxml") # This is as the name    # describes the current page we're accessing        entries = currentPage.find('div',attrs={'id':'content'}).find('div',attrs={'align':'left'}).findAll('div')    # All of our entries, it's just all divs in the curernt page that fits into    # the conditions    for entry in entries:        if entry.find('a'):            # We find all of our links in the entry and we scrape that and save            # it to a txt file            req = request.Request(str('http://degob.org/')+entry.find('a')['href'], headers = headers)            response = opener.open(req)            protocol = BeautifulSoup(response.read(),features="lxml")            fileName = protocol.find('h1').text.replace(' ','_').replace('.','_').replace('__','_')            f = open('raw-data/'+fileName+'.txt','w',encoding="utf-8")            f.write(str(protocol.decode('utf-8')))